{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import pandas_ta\n","import numpy as np\n"," \n","# from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import os\n","import time\n","import datetime\n","from six.moves import range\n","\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["PRICE_COLUMN = 'close'\n","USE_PRICE_RANGE_COLUMNS = False\n","\n","freq = 'h'\n","start_date='2016-01-02'\n","end_date='2019-03-28'\n","\n","ranges_dict_path = 'data\\\\ranges_dict.pickle'\n","save_path = f'.\\\\data\\\\featured_prices_{freq}_start_{start_date}.csv'\n","\n","# prices_path = '.\\\\data\\\\prices_freq-min_2019-01-01_2019-03-28.csv'\n","prices_path = '.\\\\data\\\\sources\\\\coinbaseUSD_1-min_data_2014-12-01_to_2019-01-09.csv'\n","\n","# Scrapped from twitters from 2016-01-01 to 2019-03-29, Collecting Tweets containing Bitcoin or BTC\n","tweets_path = 'data/sources/tweets_historical.csv'"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":"[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\Lluis\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     C:\\Users\\Lluis\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\nLoading data...\nFilling All Time data\nFilling NA data\nAggregating from min to h level\nGenerating TA features...\n83 out of 84 featuresThe following columns have been removed: ['FISHERT_120', 'KAMA_240_48_720']\nDropping 2159 rows because of NaN values\n"}],"source":["import price_features\n","\n","prices_df, ranges_dict = price_features.main(\n","    prices_path=prices_path,\n","    ranges_dict_path=ranges_dict_path,\n","    save_path=save_path,\n","    onlyRead=True,\n","    freq=freq,\n","    timestamp_col='Timestamp',\n","    cleanNans=True,\n","    start_date=start_date,\n",")"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["from tweets_preprocess import (\n","    tweetsPreprocess,\n","    VADER_COLUMNS,\n","    TEXTBLOB_COLUMNS,\n",")\n","\n","# TODO: Save tweets sentiment independent of prices and one file per date range and frequency\n","\n","sentiment_cols = VADER_COLUMNS + TEXTBLOB_COLUMNS\n","\n","save_path='data/preprocess/twitter.csv'\n","\n","partial_file = os.path.splitext(save_path)\n","save_final_path = f'{partial_file[0]}_{start_date}_-_{end_date}{partial_file[1]}'\n","\n","if os.path.exists(save_final_path):\n","    tweets_df = pd.read_csv(save_final_path, sep='\\t', index_col='timestamp')\n","\n","    tweets_df = tweets_df.set_index(\n","        pd.to_datetime(tweets_df.index)\n","    )\n","\n","else:\n","    print(\"Start tweetsPreprocess\")\n","    tweets_df = tweetsPreprocess(\n","        tweets_path,\n","        freq=freq,\n","        sentiment_cols=sentiment_cols,\n","        # sentiment_cols=['Compound', 'Polarity'],\n","        aggregate_cols=['replies', 'likes', 'retweets'], # TODO: Also by volume of tweets??\n","        start_date=start_date,\n","        end_date=end_date,\n","        nrows=None,\n","        chunksize=5e5,\n","        save_path='data/preprocess/twitter.csv',\n","        write_files=False\n","    )\n","\n","remove_cols = [\n","    'replies_sum',\n","    'replies_mean',\n","    'likes_sum',\n","    'likes_mean',\n","    'retweets_sum',\n","    'retweets_mean',\n","]\n","tweets_df = tweets_df.drop(remove_cols, axis=1)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["data = prices_df.merge(tweets_df, how='left', left_index=True, right_index=True)\n","data = data.reset_index(drop=True)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["FEATURE_COLUMNS = []\n","for key in ranges_dict:\n","    FEATURE_COLUMNS += ranges_dict[key]['cols'] if ranges_dict[key]['normalize'] else []\n","\n","FEATURE_COLUMNS += list(tweets_df.columns)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["if USE_PRICE_RANGE_COLUMNS:\n","\n","    diff_cols = len(ranges_dict['prices']['cols']) - len(FEATURE_COLUMNS) - int(POSITION_AS_OBSERVATION)\n","    print(f'Difference of {diff_cols} columns between prices cols and normalized cols')\n","    print('In order to use Group Normalization Layer with 2 groups, both groups should be equal and sorted to be one first and then the other.')\n","\n","    if diff_cols > 0:\n","        remove_cols = ['LR_14']\n","        print(f'The following columns are going to be removed: {remove_cols}')\n","        prices_cols = [col for col in ranges_dict['prices']['cols'] if col not in remove_cols]\n","    else:\n","        prices_cols = ranges_dict['prices']['cols']\n","\n","    # Add prices cols into the FEATURE_COLUMNS\n","    FEATURE_COLUMNS = prices_cols + FEATURE_COLUMNS\n","\n","# Make sure that PRICE_COL is in data\n","ALL_COLS = [PRICE_COLUMN] if PRICE_COLUMN not in FEATURE_COLUMNS else []\n","ALL_COLS += FEATURE_COLUMNS\n","\n","# Set the columns used in data PRICE_COL + FEATURE_COLS\n","data = data[ALL_COLS]"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["assert not np.isinf(data).any(1).any(), data[np.isinf(data).any(1)]\n","assert not data.isnull().any().any()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"Data for 3.050 units\n"}],"source":["# unit_factor = 60*24*30 # months \n","unit_factor = 24*30*12 # years \n","print(f'Data for {len(data.index) / unit_factor:.3f} units')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["train_time = 2\n","gap_time = 1/12\n","valid_time = (len(data.index) / unit_factor - train_time - 2 * gap_time) / 2\n","test_time = (len(data.index) / unit_factor - train_time - 2 * gap_time) / 2\n","\n","train_end = int(train_time * unit_factor)\n","valid_start = train_end + int(gap_time * unit_factor)\n","valid_end = valid_start + int(valid_time * unit_factor)\n","test_start = valid_end + int(gap_time * unit_factor)\n","test_end = test_start + int(test_time * unit_factor)\n","\n","train = data.iloc[0:train_end, :]\n","valid = data.iloc[valid_start:valid_end, :]\n","test = data.iloc[test_start:test_end, :]"]},{"cell_type":"code","execution_count":12,"metadata":{"tags":["outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend"]},"outputs":[],"source":["from own_stock_env import OwnStocksEnv, REVENUE_REWARD, PRICE_REWARD\n","\n","# TODO: Steps scheduling, starting from low number of steps to high\n","steps_schedule = [5, 10, 15, 20]\n","steps_per_episode = steps_schedule[0] # 20\n","window_size = 1\n","POSITION_AS_OBSERVATION = True\n","\n","num_parallel_environments = 1\n","\n","reward_type = PRICE_REWARD\n","max_step_reward = 0\n","max_final_reward = 1\n","\n","SEED = 12345\n","\n","#### ONLY FOR TESTING OVERFITING\n","\n","# steps_per_episode = 5\n","# factor = 2\n","# # factor = 20\n","# train = train[0:steps_per_episode*factor]\n","# valid = valid[0:steps_per_episode*factor]\n","# test = test[0:steps_per_episode*factor]\n","\n","##############################################\n","\n","# Transform Gym Environment to TFPyEnvironment\n","from gym import spaces\n","from tf_agents.environments import tf_py_environment, parallel_py_environment\n","from tf_agents.environments.gym_wrapper import GymWrapper\n","\n","def generateSplitEnvs(\n","    train_df,\n","    valid_df,\n","    test_df,\n","    window_size,\n","    steps_per_episode,\n","    feature_columns,\n","    reward_type=reward_type,\n","    max_final_reward=max_final_reward,\n","    max_step_reward=max_step_reward,\n","    num_parallel_environments=1,\n","    position_as_observation=True,\n","    constant_step=False,\n","    is_training=True,\n","    seed=12345,\n","):\n","\n","    train_env = OwnStocksEnv(\n","        df=train_df,\n","        window_size=window_size,\n","        frame_bound=(window_size, len(train_df)),\n","        steps_per_episode=steps_per_episode,\n","        is_training=True,\n","        constant_step=constant_step,\n","        feature_columns=feature_columns,\n","        position_as_observation=position_as_observation,\n","        reward_type=reward_type,\n","        max_final_reward=max_final_reward,\n","        max_step_reward=max_step_reward,\n","    )\n","    train_env.seed(seed);\n","\n","    eval_env = OwnStocksEnv(\n","        df=valid_df,\n","        window_size=window_size,\n","        frame_bound=(window_size, len(valid_df)),\n","        steps_per_episode=steps_per_episode,\n","        is_training=is_training,\n","        constant_step=constant_step,\n","        feature_columns=feature_columns,\n","        position_as_observation=position_as_observation,\n","        reward_type=reward_type,\n","        max_final_reward=max_final_reward,\n","        max_step_reward=max_step_reward,\n","    )\n","    eval_env.seed(seed);\n","\n","    test_env = OwnStocksEnv(\n","        df=test_df,\n","        window_size=window_size,\n","        frame_bound=(window_size, len(test_df)),\n","        steps_per_episode=steps_per_episode,\n","        is_training=is_training,\n","        constant_step=constant_step,\n","        feature_columns=feature_columns,\n","        position_as_observation=position_as_observation,\n","        reward_type=reward_type,\n","        max_final_reward=max_final_reward,\n","        max_step_reward=max_step_reward,\n","    )\n","    test_env.seed(seed);\n","\n","    # Otherwise raise error on evaluating ChosenActionHistogram metric\n","    spec_dtype_map = {spaces.Discrete: np.int32}\n","\n","    # TODO: Implement Parallel Environment (need tf_agents.system.multiprocessing.enable_interactive_mode() added in github last updates)\n","    if num_parallel_environments != 1:\n","        parallel_envs = []\n","        tf_parallel_envs = []\n","        for i in range(num_parallel_environments):\n","            train_env = OwnStocksEnv(\n","                df=train,\n","                window_size=window_size,\n","                frame_bound=(window_size, len(train)),\n","                steps_per_episode=steps_per_episode,\n","                is_training=True,\n","                constant_step=constant_step,\n","                feature_columns=feature_columns,\n","                position_as_observation=position_as_observation,\n","                reward_type=reward_type,\n","                max_final_reward=max_final_reward,\n","                max_step_reward=max_step_reward,\n","            )\n","            train_env.seed(SEED + i);\n","            parallel_envs.append(train_env)\n","            tf_parallel_envs.append(\n","                GymWrapper(train_env, spec_dtype_map=spec_dtype_map)\n","            )\n","        \n","        tf_env = tf_py_environment.TFPyEnvironment(parallel_py_environment.ParallelPyEnvironment(tf_parallel_envs))\n","\n","        train_env = parallel_envs[0]\n","    else:\n","        tf_env = tf_py_environment.TFPyEnvironment(GymWrapper(train_env, spec_dtype_map=spec_dtype_map))\n","\n","    eval_tf_env = tf_py_environment.TFPyEnvironment(GymWrapper(eval_env, spec_dtype_map=spec_dtype_map))\n","    test_tf_env = tf_py_environment.TFPyEnvironment(GymWrapper(test_env, spec_dtype_map=spec_dtype_map))\n","\n","    return tf_env, eval_tf_env, test_tf_env"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["tf_env, eval_tf_env, test_tf_env = generateSplitEnvs(\n","    train,\n","    valid,\n","    test,\n","    window_size,\n","    steps_per_episode,\n","    FEATURE_COLUMNS,\n","    reward_type=reward_type,\n","    max_final_reward=max_final_reward,\n","    max_step_reward=max_step_reward,\n","    num_parallel_environments=num_parallel_environments,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n","    constant_step=False,\n","    is_training=True,\n","    seed=SEED,\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["from absl import logging\n","# Added in last versions\n","# import tf_agents.system import multiprocessing\n","\n","logging.set_verbosity(logging.INFO)\n","# tf.logging.set_verbosity(tf.logging.INFO)\n","tf.compat.v1.enable_v2_behavior()\n","\n","# Added in last versions\n","# multiprocessing.enable_interactive_mode()"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["agent = 'PPO'\n","\n","STEP = 'step'\n","EPISODE = 'episode'\n","agent_unit = {\n","    'DQN': STEP,\n","    'PPO': EPISODE,\n","    'REINFORCE': EPISODE,\n","}\n","unit = agent_unit[agent]"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["from tensorflow.keras.optimizers import Adam, SGD\n","from tf_agents.utils import common\n","\n","# Params for train\n","num_iterations = 1000000\n","\n","summary_frequency = 10000\n","\n","train_steps_per_iteration = 5\n","collect_per_iteration = 20 * num_parallel_environments\n","replay_buffer_capacity = steps_per_episode * collect_per_iteration // num_parallel_environments + 1\n","\n","# TODO: Improve learning rate with schedule and on e-greedy too\n","batch_size = 32\n","learning_rate = 6e-5 # 3e-4\n","optimizer = Adam(learning_rate=learning_rate) # SGD(learning_rate=learning_rate) # Adam(learning_rate=learning_rate)\n","gradient_clipping = 0\n","\n","if agent == 'DQN':\n","    # TODO: Use other kind of policy like Boltzam?\n","    epsilon_greedy = 0.1\n","\n","    target_update_tau = 0.05\n","    target_update_period = 5\n","\n","    initial_collect_steps = num_iterations // 1000 # 1000\n","\n","    n_step_update = 1\n","\n","    td_errors_loss_fn = common.element_wise_huber_loss # common.element_wise_squared_loss # common.element_wise_huber_loss\n","\n","    gamma = 0.99\n","    reward_scale_factor = 1.0\n","\n","elif agent == 'PPO':\n","    \n","    importance_ratio_clipping = 0.2\n","    \n","    kl_cutoff_factor = 0 # 2.0\n","    kl_cutoff_coef = 1000.0\n","    initial_adaptive_kl_beta = 0 # 1.0\n","    adaptive_kl_target = 0.01\n","    adaptive_kl_tolerance = 0.3\n","\n","    normalize_observations=True\n","    normalize_rewards=True\n","    reward_norm_clipping=10.0 # Not used if normalize_rewards=False\n","    use_gae=True\n","    lambda_value=1 # 0.95 \n","    discount_factor=1 # TODO: Rethink on how to implement discount factor because reward by prices is accumulative\n","\n","    entropy_regularization = 0\n","    policy_l2_reg = 0\n","    value_function_l2_reg = 0\n","    shared_vars_l2_reg = 0\n","    value_pred_loss_coef = 0.5\n","    use_td_lambda_return = False\n","    log_prob_clipping = 0.0\n","    value_clipping = None\n","    num_epochs = 25\n","\n","use_tf_functions = True\n","\n","# Params for summaries and logging\n","log_interval = num_iterations // summary_frequency\n","log_interval = max(log_interval, steps_per_episode)\n","summaries_flush_secs = 10\n","summary_interval = num_iterations // summary_frequency\n","summary_interval = max(summary_interval, steps_per_episode)\n","debug_summaries = True\n","summarize_grads_and_vars = True\n","check_numerics = True\n","\n","# Params for eval\n","num_eval_episodes = eval_tf_env.envs[0].frame_bound[-1] // eval_tf_env.envs[0].steps_per_episode\n","num_eval_seeds = 1\n","eval_interval = summary_interval * 4\n","eval_interval = max(eval_interval, steps_per_episode)\n","eval_metrics_callback = None\n","\n","# Params for checkpoints\n","train_checkpoint_interval = eval_interval * 4\n","policy_checkpoint_interval = eval_interval * 2\n","rb_checkpoint_interval = eval_interval * 8"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# !rmdir /s /q .\\\\logs\\\\dqn"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["TRAIN_MODEL = True\n","\n","root_dir = 'logs\\\\' + agent\n","\n","if TRAIN_MODEL:\n","    root_dir = os.path.join(root_dir, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n","else:\n","    root_dir = os.path.join(root_dir, '20200503-095638')"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["class AgentEarlyStopping():\n","  def __init__(self,\n","               monitor='AverageReturn',\n","               min_delta=0,\n","               patience=0,\n","               patience_after_change=0,\n","               verbose=0,\n","               mode='max',\n","               baseline=None):\n","    \"\"\"Initialize an AgentEarlyStopping.\n","    Arguments:\n","        monitor: Quantity to be monitored.\n","        min_delta: Minimum change in the monitored quantity\n","            to qualify as an improvement, i.e. an absolute\n","            change of less than min_delta, will count as no\n","            improvement.\n","        patience: Number of iterations with no improvement\n","            after which training will be stopped.\n","        patience_after_change: Number of iterations after change\n","             on monitor with no improvement after which training\n","             will be stopped.\n","        verbose: verbosity mode.\n","        mode: One of `{\"auto\", \"min\", \"max\"}`. In `min` mode,\n","            training will stop when the quantity\n","            monitored has stopped decreasing; in `max`\n","            mode it will stop when the quantity\n","            monitored has stopped increasing; in `auto`\n","            mode, the direction is automatically inferred\n","            from the name of the monitored quantity.\n","        baseline: Baseline value for the monitored quantity.\n","            Training will stop if the model doesn't show improvement over the\n","            baseline.\n","    \"\"\"\n","    # super(AgentEarlyStopping, self).__init__()\n","\n","    self.monitor = monitor\n","    self.patience = patience\n","    if patience_after_change < patience:\n","        self.patience_after_change = patience\n","    else:\n","        self.patience_after_change = patience_after_change\n","    self.verbose = verbose\n","    self.baseline = baseline\n","    self.min_delta = abs(min_delta)\n","\n","    if mode not in ['auto', 'min', 'max']:\n","      logging.warning('EarlyStopping mode %s is unknown, '\n","                      'fallback to auto mode.', mode)\n","      mode = 'auto'\n","\n","    if mode == 'min':\n","      self.monitor_op = np.less\n","    elif mode == 'max':\n","      self.monitor_op = np.greater\n","    else:\n","      if 'acc' in self.monitor:\n","        self.monitor_op = np.greater\n","      elif 'return' in self.monitor.lower():\n","        self.monitor_op = np.less\n","      else:\n","        self.monitor_op = np.less\n","\n","    if self.monitor_op == np.greater:\n","      self.min_delta *= 1\n","    else:\n","      self.min_delta *= -1\n","\n","    self.reset()\n","\n","  def reset(self):\n","    # Allow instances to be re-used\n","    self.wait = 0\n","    self.wait_after_change = 0\n","    self.stopped_step = 0\n","    self.stop_training = False\n","    self.monitor_changed = False\n","    if self.baseline is not None:\n","      self.best = self.baseline\n","    else:\n","      self.best = np.Inf if self.monitor_op == np.less else -np.Inf\n","\n","  def __call__(self, computed_metrics, global_step):\n","    current = self.get_monitor_value(computed_metrics)\n","    if current is None:\n","      return\n","    if not tf.equal(current, self.best) and not np.isinf(self.best):\n","        self.monitor_changed = True\n","    if self.monitor_op(current - self.min_delta, self.best):\n","      self.best = current\n","      self.wait = 0\n","      self.wait_after_change = 0\n","    else:\n","      self.wait += 1\n","      if self.monitor_changed:\n","          self.wait_after_change += 1\n","          print(f'self.wait_after_change: {self.wait_after_change}')\n","      if self.wait >= self.patience or self.wait_after_change >= self.patience_after_change:\n","        self.stopped_step = global_step\n","        self.stop_training = True\n","        logging.info('Global step %05d: early stopping' % (self.stopped_step + 1))\n","\n","  def get_monitor_value(self, computed_metrics):\n","    computed_metrics = computed_metrics or {}\n","    monitor_value = computed_metrics.get(self.monitor).numpy()\n","    if monitor_value is None:\n","      logging.warning('Agent early stopping conditioned on metric `%s` '\n","                      'which is not available. Available metrics are: %s',\n","                      self.monitor, ','.join(list(computed_metrics.keys())))\n","    return monitor_value"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["from tf_agents.metrics import tf_metrics\n","\n","root_dir = os.path.expanduser(root_dir)\n","train_dir = os.path.join(root_dir, 'train')\n","eval_dir = os.path.join(root_dir, 'eval')\n","saved_model_dir = os.path.join(root_dir, 'policy_saved_model')\n","\n","train_summary_writer = tf.summary.create_file_writer(\n","    train_dir, flush_millis=summaries_flush_secs * 1000)\n","train_summary_writer.set_as_default()\n","\n","step_metrics = []\n","train_metrics = step_metrics + [\n","    # tf_metrics.NumberOfEpisodes(),\n","    # tf_metrics.EnvironmentSteps(),\n","    tf_metrics.AverageReturnMetric(batch_size=num_parallel_environments),\n","    # tf_metrics.AverageEpisodeLengthMetric(),\n","    # tf_metrics.ChosenActionHistogram(dtype=tf.int32),\n","]\n","\n","eval_summary_writer = tf.summary.create_file_writer(\n","    eval_dir, flush_millis=summaries_flush_secs * 1000)\n","eval_metrics = [\n","    tf_metrics.AverageReturnMetric(buffer_size=1),\n","    # tf_metrics.AverageEpisodeLengthMetric(buffer_size=num_eval_episodes)\n","]\n","eval_metrics_callback = AgentEarlyStopping(\n","    monitor='AverageReturn', min_delta=0.0001, patience=np.inf, patience_after_change=5, verbose=1, mode='max'\n",")\n","\n","global_step = tf.compat.v1.train.get_or_create_global_step()"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["import collections\n","\n","def evaluate(eval_metrics, eval_tf_env, eval_policy, num_eval_episodes, num_eval_seeds, global_step=None, eval_summary_writer=None, summary_prefix='Metrics', seed=12345):\n","    all_results = []\n","    for i in range(num_eval_seeds):\n","        for env in eval_tf_env.envs:\n","            env.seed(seed + i)\n","        # One final eval before exiting.\n","        results = metric_utils.eager_compute(\n","            eval_metrics,\n","            eval_tf_env,\n","            eval_policy,\n","            num_episodes=num_eval_episodes,\n","            train_step=global_step,\n","        )\n","        all_results.append(results)\n","\n","    mean_results = collections.OrderedDict(results)\n","    if num_eval_seeds > 1:\n","        for metric in mean_results:\n","            metric_sum = 0\n","            for result in all_results:\n","                metric_sum = tf.add(metric_sum, result[metric])\n","            mean_results[metric] = metric_sum / len(all_results)\n","    if global_step and eval_summary_writer:\n","        with eval_summary_writer.as_default():\n","            for metric, value in mean_results.items():\n","                tag = common.join_scope(summary_prefix, metric)\n","                tf.compat.v2.summary.scalar(name=tag, data=value, step=global_step)\n","\n","    log = ['{0} = {1}'.format(metric, value) for metric, value in mean_results.items()]\n","    logging.info('%s \\n\\t\\t %s','', '\\n\\t\\t '.join(log))\n","\n","    return mean_results"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[],"source":["# Define Q-network\n","\n","train_sequence_length = window_size\n","\n","# dropout_layer = (0.2,0.2,0.2,0.2,0.2)\n","dropout_layer = None\n","activation_fn = tf.nn.leaky_relu # tf.keras.activations.relu # tf.keras.activations.tanh\n","\n","if agent == 'DQN':\n","    if train_sequence_length > 1:\n","        input_fc_layer_params = (8,)\n","        lstm_size = (16,)\n","        output_fc_layer_params = (8,)\n","        \n","    else:\n","        fc_layer_params = (100,)\n","\n","elif agent == 'PPO':\n","    if train_sequence_length > 1:\n","        actor_fc_layers = (8,)\n","        actor_lstm_size = (16,)\n","        actor_output_fc_layer = (8,)\n","        \n","        value_fc_layers = (8,)\n","        value_lstm_size = (16,)\n","        value_output_fc_layers = (8,)\n","    else:\n","        actor_fc_layers = (512,1024,2048,1024,512,)\n","        \n","        value_fc_layers = (512,1024,2048,1024,512,)\n","\n","\n","if agent == 'DQN':\n","    from tf_agents.networks import q_network\n","    from tf_agents.networks import q_rnn_network\n","\n","    if train_sequence_length > 1:\n","        q_net = q_rnn_network.QRnnNetwork(\n","            tf_env.observation_spec(),\n","            tf_env.action_spec(),\n","            input_fc_layer_params=input_fc_layer_params,\n","            lstm_size=lstm_size,\n","            output_fc_layer_params=output_fc_layer_params\n","        )\n","    else:\n","        q_net = q_network.QNetwork(\n","            tf_env.observation_spec(),\n","            tf_env.action_spec(),\n","            fc_layer_params=fc_layer_params,\n","            dropout_layer_params=dropout_layer,\n","        )\n","        train_sequence_length = n_step_update\n","\n","    if train_sequence_length != 1 and n_step_update != 1:\n","        raise NotImplementedError(\n","            'Currently not supporting n-step updates with stateful networks (i.e., RNNs)')\n","\n","elif agent == 'PPO':\n","    from tf_agents.networks import actor_distribution_network\n","    from tf_agents.networks import actor_distribution_rnn_network\n","    from tf_agents.networks import value_network\n","    from tf_agents.networks import value_rnn_network\n","\n","    if train_sequence_length > 1:\n","        actor_net = actor_distribution_rnn_network.ActorDistributionRnnNetwork(\n","            tf_env.observation_spec(),\n","            tf_env.action_spec(),\n","            input_fc_layer_params=actor_fc_layers,\n","            input_dropout_layer_params=dropout_layer,\n","            lstm_size=actor_lstm_size,\n","            activation_fn=activation_fn,\n","            output_fc_layer_params=actor_output_fc_layer)\n","        value_net = value_rnn_network.ValueRnnNetwork(\n","            tf_env.observation_spec(),\n","            input_fc_layer_params=value_fc_layers,\n","            input_dropout_layer_params=dropout_layer,\n","            lstm_size=value_lstm_size,\n","            activation_fn=activation_fn, # alredy relu on source code\n","            output_fc_layer_params=actor_output_fc_layer)\n","    else:\n","        actor_net = actor_distribution_network.ActorDistributionNetwork(\n","            tf_env.observation_spec(),\n","            tf_env.action_spec(),\n","            fc_layer_params=actor_fc_layers,\n","            dropout_layer_params=dropout_layer,\n","            activation_fn=activation_fn)\n","        value_net = value_network.ValueNetwork(\n","            tf_env.observation_spec(),\n","            fc_layer_params=value_fc_layers,\n","            dropout_layer_params=dropout_layer,\n","            activation_fn=activation_fn)"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[],"source":["def train_eval(tf_agent, num_iterations, batch_size, tf_env, eval_tf_env, train_metrics, step_metrics, eval_metrics, global_step, replay_buffer_capacity, num_parallel_environments, collect_per_iteration, train_steps_per_iteration, train_dir, saved_model_dir, eval_summary_writer, num_eval_episodes, num_eval_seeds=1, eval_metrics_callback=None, train_sequence_length=1, initial_collect_steps=1000, train_model=True, use_tf_functions=True, eval_early_stopping=False):\n","\n","    tf_agent.initialize()\n","    agent_name = tf_agent.__dict__['_name']\n","\n","    eval_policy = tf_agent.policy\n","    collect_policy = tf_agent.collect_policy\n","\n","    replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n","        data_spec=tf_agent.collect_data_spec,\n","        batch_size=num_parallel_environments, # batch_size=tf_env.batch_size,\n","        max_length=replay_buffer_capacity)\n","\n","    if train_model:\n","      if agent_name in ['dqn_agent']:\n","        collect_driver = dynamic_step_driver.DynamicStepDriver(\n","            tf_env,\n","            collect_policy,\n","            observers=[replay_buffer.add_batch] + train_metrics,\n","            num_steps=collect_per_iteration)\n","      elif agent_name in ['ppo_agent']:\n","        collect_driver = dynamic_episode_driver.DynamicEpisodeDriver(\n","            tf_env,\n","            collect_policy,\n","            observers=[replay_buffer.add_batch] + train_metrics,\n","            num_episodes=collect_per_iteration)\n","      else:\n","          raise NotImplementedError(f'{agent_name} agent not yet implemented')\n","\n","    train_checkpointer = common.Checkpointer(\n","        ckpt_dir=train_dir,\n","        agent=tf_agent,\n","        global_step=global_step,\n","        metrics=metric_utils.MetricsGroup(train_metrics, 'train_metrics'))\n","    policy_checkpointer = common.Checkpointer(\n","        ckpt_dir=os.path.join(train_dir, 'policy'),\n","        policy=eval_policy,\n","        global_step=global_step)\n","    saved_model = policy_saver.PolicySaver(eval_policy, train_step=global_step)\n","    rb_checkpointer = common.Checkpointer(\n","        ckpt_dir=os.path.join(train_dir, 'replay_buffer'),\n","        max_to_keep=1,\n","        replay_buffer=replay_buffer)\n","\n","    policy_checkpointer.initialize_or_restore() # TODO: To be tested\n","    train_checkpointer.initialize_or_restore()\n","    rb_checkpointer.initialize_or_restore()\n","\n","    if train_model:\n","\n","      # TODO: should they use autograph=False?? as in tf_agents/agents/ppo/examples/v2/train_eval_clip_agent.py\n","      if use_tf_functions:\n","        # To speed up collect use common.function.\n","        collect_driver.run = common.function(collect_driver.run) \n","        tf_agent.train = common.function(tf_agent.train)\n","\n","      # Only run Replay buffer initialization if using one of the following agents\n","      if agent_name in ['dqn_agent']:\n","        initial_collect_policy = random_tf_policy.RandomTFPolicy(\n","            tf_env.time_step_spec(), tf_env.action_spec())\n","\n","        # Collect initial replay data.\n","        logging.info(\n","            'Initializing replay buffer by collecting experience for %d steps with '\n","            'a random policy.', initial_collect_steps)\n","        dynamic_step_driver.DynamicStepDriver(\n","            tf_env,\n","            initial_collect_policy,\n","            observers=[replay_buffer.add_batch] + train_metrics,\n","            num_steps=initial_collect_steps).run()\n","\n","      logging.info(\n","          f'Initial eval metric'\n","      )\n","      results = evaluate(eval_metrics, eval_tf_env, eval_policy, num_eval_episodes, num_eval_seeds, global_step, eval_summary_writer, summary_prefix='Metrics', seed=SEED)\n","\n","      if eval_early_stopping and not isinstance(eval_metrics_callback, AgentEarlyStopping):\n","          raise ValueError('Cannot set eval_early_stopping without eval_metric_callback being Agent Early Stopping instance')\n","\n","      if eval_metrics_callback is not None:\n","        eval_metrics_callback(results, global_step.numpy())\n","\n","      time_step = None\n","      policy_state = collect_policy.get_initial_state(tf_env.batch_size)\n","\n","      timed_at_step = global_step.numpy()\n","      collect_time = 0\n","      train_time = 0\n","      summary_time = 0\n","\n","      if agent_name in ['dqn_agent']:\n","        # Dataset generates trajectories with shape [Bx2x...]\n","        logging.info(\n","            f'Dataset generates trajectories'\n","        )\n","        dataset = replay_buffer.as_dataset(\n","            num_parallel_calls=3,\n","            sample_batch_size=batch_size,\n","            # single_deterministic_pass=True,\n","            num_steps=train_sequence_length + 1).prefetch(3)\n","        iterator = iter(dataset)\n","\n","        def train_step():\n","          experience, _ = next(iterator)\n","          return tf_agent.train(experience)\n","      elif agent_name in ['ppo_agent']:\n","        def train_step():\n","          trajectories = replay_buffer.gather_all()\n","          return tf_agent.train(experience=trajectories)\n","      else:\n","        raise NotImplementedError(f'{agent_name} agent not yet implemented')\n","\n","      if use_tf_functions:\n","        train_step = common.function(train_step)\n","\n","      logging.info(\n","            f'Starting training...'\n","      )\n","      for _ in range(num_iterations):\n","        start_time = time.time()\n","        if agent_name in ['dqn_agent']:\n","          time_step, policy_state = collect_driver.run(\n","              time_step=time_step,\n","              policy_state=policy_state,\n","          )\n","        elif agent_name in ['ppo_agent']:\n","          collect_driver.run()\n","        else:\n","          raise NotImplementedError(f'{agent_name} agent not yet implemented')\n","        \n","        collect_time += time.time() - start_time\n","\n","        start_time = time.time()\n","        for _ in range(train_steps_per_iteration):\n","          train_loss = train_step()\n","        train_time += time.time() - start_time\n","\n","        start_time = time.time()\n","        for train_metric in train_metrics:\n","          train_metric.tf_summaries(\n","              train_step=global_step, step_metrics=step_metrics)\n","        summary_time += time.time() - start_time\n","\n","        if global_step.numpy() % log_interval == 0:\n","          logging.info('step = %d, loss = %f', global_step.numpy(),\n","                      train_loss.loss)\n","          steps_per_sec = (global_step.numpy() - timed_at_step) / (train_time + collect_time + summary_time)\n","          logging.info('%.3f steps/sec', steps_per_sec)\n","          logging.info('collect_time = %.3f, train_time = %.3f, summary_time = %.3f', collect_time,\n","                     train_time, summary_time)\n","          tf.compat.v2.summary.scalar(\n","              name='global_steps_per_sec', data=steps_per_sec, step=global_step)\n","          timed_at_step = global_step.numpy()\n","          collect_time = 0\n","          train_time = 0\n","          summary_time = 0\n","\n","        if global_step.numpy() % train_checkpoint_interval == 0:\n","          start_time = time.time()\n","          train_checkpointer.save(global_step=global_step.numpy())\n","          logging.info(\n","            f'Saving Train lasts: {time.time() - start_time:.3f} s'\n","          )\n","\n","        if global_step.numpy() % policy_checkpoint_interval == 0:\n","          start_time = time.time()\n","          policy_checkpointer.save(global_step=global_step.numpy())\n","          saved_model_path = os.path.join(\n","              saved_model_dir, 'policy_' + ('%d' % global_step.numpy()).zfill(9))\n","          saved_model.save(saved_model_path)\n","          logging.info(\n","            f'Saving Policy lasts: {time.time() - start_time:.3f} s'\n","          )\n","\n","        if global_step.numpy() % rb_checkpoint_interval == 0:\n","          start_time = time.time()\n","          rb_checkpointer.save(global_step=global_step.numpy())\n","          logging.info(\n","            f'Saving Replay Buffer lasts: {time.time() - start_time:.3f} s'\n","          )\n","\n","        if global_step.numpy() % eval_interval == 0:\n","          start_time = time.time()\n","          results = evaluate(eval_metrics, eval_tf_env, eval_policy, num_eval_episodes, num_eval_seeds, global_step, eval_summary_writer, summary_prefix='Metrics', seed=SEED)\n","          if eval_metrics_callback is not None:\n","            eval_metrics_callback(results, global_step.numpy())\n","          logging.info(\n","            f'Calculate Evaluation lasts {time.time() - start_time:.3f} s'\n","          )\n","\n","          if eval_early_stopping and eval_metrics_callback.stop_training:\n","              logging.info(\n","                  f'Training stopped due to Agent Early Stopping at step: {global_step.numpy()}'\n","              )\n","              logging.info(\n","                  f'Best {eval_metrics_callback.monitor} was {eval_metrics_callback.best:.5f} at step {eval_metrics_callback.stopped_step}'\n","              )\n","              break"]},{"cell_type":"code","execution_count":24,"metadata":{"tags":["outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend"]},"outputs":[{"output_type":"stream","name":"stderr","text":"INFO:absl:Steps per episode equal to 5\nINFO:absl:No checkpoint available at logs\\PPO\\20200518-173612\\train\nINFO:absl:No checkpoint available at logs\\PPO\\20200518-173612\\train\\policy\nINFO:absl:No checkpoint available at logs\\PPO\\20200518-173612\\train\\replay_buffer\nINFO:absl:Initial eval metric\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Starting training...\nINFO:absl:step = 500, loss = 0.035012\nINFO:absl:4.609 steps/sec\nINFO:absl:collect_time = 1.684, train_time = 106.728, summary_time = 0.075\nINFO:absl:step = 1000, loss = -0.045781\nINFO:absl:19.369 steps/sec\nINFO:absl:collect_time = 1.106, train_time = 24.701, summary_time = 0.007\nINFO:absl:step = 1500, loss = -0.113828\nINFO:absl:19.443 steps/sec\nINFO:absl:collect_time = 1.086, train_time = 24.618, summary_time = 0.012\nINFO:absl:step = 2000, loss = 0.050401\nINFO:absl:17.271 steps/sec\nINFO:absl:collect_time = 1.054, train_time = 27.891, summary_time = 0.005\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Calculate Evaluation lasts 10.178 s\nINFO:absl:step = 2500, loss = -0.026235\nINFO:absl:19.066 steps/sec\nINFO:absl:collect_time = 1.113, train_time = 25.107, summary_time = 0.005\nINFO:absl:step = 3000, loss = -0.028783\nINFO:absl:19.102 steps/sec\nINFO:absl:collect_time = 1.096, train_time = 25.072, summary_time = 0.007\nINFO:absl:step = 3500, loss = 0.027115\nINFO:absl:19.250 steps/sec\nINFO:absl:collect_time = 1.065, train_time = 24.899, summary_time = 0.010\nINFO:absl:step = 4000, loss = 0.000164\nINFO:absl:19.133 steps/sec\nINFO:absl:collect_time = 1.131, train_time = 24.997, summary_time = 0.006\nINFO:absl:Saved checkpoint: logs\\PPO\\20200518-173612\\train\\policy\\ckpt-4000\nWARNING:tensorflow:From C:\\Users\\Lluis\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nWARNING:tensorflow:From C:\\Users\\Lluis\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\nInstructions for updating:\nIf using Keras pass *_constraint arguments to layers.\nINFO:tensorflow:Assets written to: logs\\PPO\\20200518-173612\\policy_saved_model\\policy_000004000\\assets\nINFO:tensorflow:Assets written to: logs\\PPO\\20200518-173612\\policy_saved_model\\policy_000004000\\assets\nINFO:absl:Saving Policy lasts: 3.158 s\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Calculate Evaluation lasts 10.239 s\nINFO:absl:step = 4500, loss = -0.041685\nINFO:absl:18.106 steps/sec\nINFO:absl:collect_time = 1.078, train_time = 26.531, summary_time = 0.006\nINFO:absl:step = 5000, loss = -0.009000\nINFO:absl:17.424 steps/sec\nINFO:absl:collect_time = 1.141, train_time = 27.547, summary_time = 0.008\nINFO:absl:step = 5500, loss = -0.024923\nINFO:absl:19.383 steps/sec\nINFO:absl:collect_time = 1.085, train_time = 24.698, summary_time = 0.012\nINFO:absl:step = 6000, loss = 0.000272\nINFO:absl:19.031 steps/sec\nINFO:absl:collect_time = 1.204, train_time = 25.061, summary_time = 0.008\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Calculate Evaluation lasts 10.069 s\nINFO:absl:step = 6500, loss = -0.029140\nINFO:absl:18.703 steps/sec\nINFO:absl:collect_time = 1.128, train_time = 25.597, summary_time = 0.008\nINFO:absl:step = 7000, loss = 0.013774\nINFO:absl:18.031 steps/sec\nINFO:absl:collect_time = 1.144, train_time = 26.579, summary_time = 0.007\nINFO:absl:step = 7500, loss = -0.041689\nINFO:absl:18.895 steps/sec\nINFO:absl:collect_time = 1.171, train_time = 25.281, summary_time = 0.010\nINFO:absl:step = 8000, loss = 0.002447\nINFO:absl:18.930 steps/sec\nINFO:absl:collect_time = 1.087, train_time = 25.320, summary_time = 0.007\nINFO:absl:Saved checkpoint: logs\\PPO\\20200518-173612\\train\\ckpt-8000\nINFO:absl:Saving Train lasts: 3.289 s\nINFO:absl:Saved checkpoint: logs\\PPO\\20200518-173612\\train\\policy\\ckpt-8000\nINFO:tensorflow:Assets written to: logs\\PPO\\20200518-173612\\policy_saved_model\\policy_000008000\\assets\nINFO:tensorflow:Assets written to: logs\\PPO\\20200518-173612\\policy_saved_model\\policy_000008000\\assets\nINFO:absl:Saving Policy lasts: 4.806 s\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Calculate Evaluation lasts 10.191 s\nINFO:absl:step = 8500, loss = 0.018542\nINFO:absl:17.297 steps/sec\nINFO:absl:collect_time = 1.161, train_time = 27.737, summary_time = 0.008\nINFO:absl:step = 9000, loss = -0.070019\nINFO:absl:18.643 steps/sec\nINFO:absl:collect_time = 1.083, train_time = 25.730, summary_time = 0.006\nINFO:absl:step = 9500, loss = -0.035417\nINFO:absl:18.684 steps/sec\nINFO:absl:collect_time = 1.056, train_time = 25.694, summary_time = 0.010\nINFO:absl:step = 10000, loss = -0.028308\nINFO:absl:19.058 steps/sec\nINFO:absl:collect_time = 1.099, train_time = 25.129, summary_time = 0.008\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Calculate Evaluation lasts 9.975 s\nINFO:absl:step = 10500, loss = -0.019981\nINFO:absl:18.508 steps/sec\nINFO:absl:collect_time = 1.117, train_time = 25.890, summary_time = 0.008\nINFO:absl:step = 11000, loss = -0.034646\nINFO:absl:19.159 steps/sec\nINFO:absl:collect_time = 1.114, train_time = 24.976, summary_time = 0.007\nINFO:absl:step = 11500, loss = -0.009728\nINFO:absl:18.761 steps/sec\nINFO:absl:collect_time = 1.191, train_time = 25.449, summary_time = 0.011\nINFO:absl:step = 12000, loss = 0.042817\nINFO:absl:18.825 steps/sec\nINFO:absl:collect_time = 1.262, train_time = 25.291, summary_time = 0.008\nINFO:absl:Saved checkpoint: logs\\PPO\\20200518-173612\\train\\policy\\ckpt-12000\nINFO:tensorflow:Assets written to: logs\\PPO\\20200518-173612\\policy_saved_model\\policy_000012000\\assets\nINFO:tensorflow:Assets written to: logs\\PPO\\20200518-173612\\policy_saved_model\\policy_000012000\\assets\nINFO:absl:Saving Policy lasts: 3.178 s\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Calculate Evaluation lasts 10.012 s\nINFO:absl:step = 12500, loss = -0.004466\nINFO:absl:19.027 steps/sec\nINFO:absl:collect_time = 1.165, train_time = 25.107, summary_time = 0.007\nINFO:absl:step = 13000, loss = -0.017083\nINFO:absl:19.030 steps/sec\nINFO:absl:collect_time = 1.104, train_time = 25.163, summary_time = 0.008\nINFO:absl:step = 13500, loss = 0.049880\nINFO:absl:18.908 steps/sec\nINFO:absl:collect_time = 1.105, train_time = 25.329, summary_time = 0.010\nINFO:absl:step = 14000, loss = 0.006758\nINFO:absl:18.941 steps/sec\nINFO:absl:collect_time = 1.121, train_time = 25.269, summary_time = 0.007\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Calculate Evaluation lasts 10.028 s\nINFO:absl:step = 14500, loss = 0.023340\nINFO:absl:19.111 steps/sec\nINFO:absl:collect_time = 1.160, train_time = 24.998, summary_time = 0.006\nINFO:absl:step = 15000, loss = 0.091552\nINFO:absl:19.099 steps/sec\nINFO:absl:collect_time = 1.076, train_time = 25.096, summary_time = 0.007\nINFO:absl:step = 15500, loss = 0.094874\nINFO:absl:19.013 steps/sec\nINFO:absl:collect_time = 1.093, train_time = 25.195, summary_time = 0.010\nINFO:absl:step = 16000, loss = 0.090181\nINFO:absl:19.049 steps/sec\nINFO:absl:collect_time = 1.095, train_time = 25.148, summary_time = 0.005\nINFO:absl:Saved checkpoint: logs\\PPO\\20200518-173612\\train\\ckpt-16000\nINFO:absl:Saving Train lasts: 3.477 s\nINFO:absl:Saved checkpoint: logs\\PPO\\20200518-173612\\train\\policy\\ckpt-16000\nINFO:tensorflow:Assets written to: logs\\PPO\\20200518-173612\\policy_saved_model\\policy_000016000\\assets\nINFO:tensorflow:Assets written to: logs\\PPO\\20200518-173612\\policy_saved_model\\policy_000016000\\assets\nINFO:absl:Saving Policy lasts: 3.098 s\nINFO:absl:Saved checkpoint: logs\\PPO\\20200518-173612\\train\\replay_buffer\\ckpt-16000\nINFO:absl:Saving Replay Buffer lasts: 0.041 s\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Calculate Evaluation lasts 10.356 s\nINFO:absl:step = 16500, loss = 0.051518\nINFO:absl:18.445 steps/sec\nINFO:absl:collect_time = 1.155, train_time = 25.945, summary_time = 0.008\nINFO:absl:step = 17000, loss = -0.003573\nINFO:absl:18.815 steps/sec\nINFO:absl:collect_time = 1.096, train_time = 25.470, summary_time = 0.008\nINFO:absl:step = 17500, loss = -0.045874\nINFO:absl:18.783 steps/sec\nINFO:absl:collect_time = 1.149, train_time = 25.463, summary_time = 0.009\nINFO:absl:step = 18000, loss = 0.072102\nINFO:absl:18.689 steps/sec\nINFO:absl:collect_time = 1.106, train_time = 25.643, summary_time = 0.004\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Calculate Evaluation lasts 10.068 s\nINFO:absl:step = 18500, loss = 0.033280\nINFO:absl:18.928 steps/sec\nINFO:absl:collect_time = 1.085, train_time = 25.323, summary_time = 0.008\nINFO:absl:step = 19000, loss = -0.068985\nINFO:absl:19.129 steps/sec\nINFO:absl:collect_time = 1.087, train_time = 25.045, summary_time = 0.006\nINFO:absl:step = 19500, loss = 0.051993\nINFO:absl:18.807 steps/sec\nINFO:absl:collect_time = 1.101, train_time = 25.473, summary_time = 0.012\nINFO:absl:step = 20000, loss = 0.018852\nINFO:absl:18.784 steps/sec\nINFO:absl:collect_time = 1.154, train_time = 25.459, summary_time = 0.006\nINFO:absl:Saved checkpoint: logs\\PPO\\20200518-173612\\train\\policy\\ckpt-20000\nINFO:tensorflow:Assets written to: logs\\PPO\\20200518-173612\\policy_saved_model\\policy_000020000\\assets\nINFO:tensorflow:Assets written to: logs\\PPO\\20200518-173612\\policy_saved_model\\policy_000020000\\assets\nINFO:absl:Saving Policy lasts: 3.006 s\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Calculate Evaluation lasts 10.748 s\nINFO:absl:step = 20500, loss = 0.008491\nINFO:absl:17.494 steps/sec\nINFO:absl:collect_time = 1.149, train_time = 27.424, summary_time = 0.008\nINFO:absl:step = 21000, loss = -0.034895\nINFO:absl:18.004 steps/sec\nINFO:absl:collect_time = 1.157, train_time = 26.606, summary_time = 0.008\nINFO:absl:step = 21500, loss = -0.049317\nINFO:absl:19.627 steps/sec\nINFO:absl:collect_time = 1.092, train_time = 24.372, summary_time = 0.011\nINFO:absl:step = 22000, loss = -0.008215\nINFO:absl:18.472 steps/sec\nINFO:absl:collect_time = 1.140, train_time = 25.921, summary_time = 0.007\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Calculate Evaluation lasts 10.038 s\nINFO:absl:step = 22500, loss = 0.024952\nINFO:absl:19.097 steps/sec\nINFO:absl:collect_time = 1.149, train_time = 25.026, summary_time = 0.007\nINFO:absl:step = 23000, loss = 0.052950\nINFO:absl:18.927 steps/sec\nINFO:absl:collect_time = 1.128, train_time = 25.284, summary_time = 0.006\nINFO:absl:step = 23500, loss = 0.029550\nINFO:absl:18.433 steps/sec\nINFO:absl:collect_time = 1.182, train_time = 25.932, summary_time = 0.012\nINFO:absl:step = 24000, loss = -0.049012\nINFO:absl:19.007 steps/sec\nINFO:absl:collect_time = 1.172, train_time = 25.128, summary_time = 0.007\nINFO:absl:Saved checkpoint: logs\\PPO\\20200518-173612\\train\\ckpt-24000\nINFO:absl:Saving Train lasts: 3.432 s\nINFO:absl:Saved checkpoint: logs\\PPO\\20200518-173612\\train\\policy\\ckpt-24000\nINFO:tensorflow:Assets written to: logs\\PPO\\20200518-173612\\policy_saved_model\\policy_000024000\\assets\nINFO:tensorflow:Assets written to: logs\\PPO\\20200518-173612\\policy_saved_model\\policy_000024000\\assets\nINFO:absl:Saving Policy lasts: 3.109 s\nINFO:absl: \n\t\t AverageReturn = 0.0\nINFO:absl:Calculate Evaluation lasts 10.300 s\nINFO:absl:step = 24500, loss = 0.063156\nINFO:absl:18.820 steps/sec\nINFO:absl:collect_time = 1.168, train_time = 25.391, summary_time = 0.009\nINFO:absl:step = 25000, loss = -0.087009\nINFO:absl:19.056 steps/sec\nINFO:absl:collect_time = 1.094, train_time = 25.136, summary_time = 0.009\n"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-24-7f9927798248>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mtrain_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTRAIN_MODEL\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0muse_tf_functions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_tf_functions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0meval_early_stopping\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m       )\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-23-283aa650fab9>\u001b[0m in \u001b[0;36mtrain_eval\u001b[1;34m(tf_agent, num_iterations, batch_size, tf_env, eval_tf_env, train_metrics, step_metrics, eval_metrics, global_step, replay_buffer_capacity, num_parallel_environments, collect_per_iteration, train_steps_per_iteration, train_dir, saved_model_dir, eval_summary_writer, num_eval_episodes, num_eval_seeds, eval_metrics_callback, train_sequence_length, initial_collect_steps, train_model, use_tf_functions, eval_early_stopping)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_steps_per_iteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m           \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m         \u001b[0mtrain_time\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    604\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m       \u001b[1;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    607\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# TODO: Adapt for using step or episodes as unit to then can switch easily between TF-Agents\n","# Compare here: https://github.com/tensorflow/agents/blob/master/tf_agents/agents/ppo/examples/v2/train_eval_clip_agent.py\n","from tf_agents.agents.dqn import dqn_agent\n","from tf_agents.agents.ppo import ppo_agent # TODO: Use ppo_clip_agent which is the proposed above\n","from tf_agents.drivers import dynamic_step_driver, dynamic_episode_driver\n","from tf_agents.eval import metric_utils\n","from tf_agents.policies import random_tf_policy\n","from tf_agents.replay_buffers import tf_uniform_replay_buffer\n","from tf_agents.utils import common\n","\n","from tf_agents.policies import policy_saver\n","\n","with tf.summary.record_if(\n","    lambda: tf.math.equal(global_step % summary_interval, 0)):\n","\n","    if agent == 'DQN':\n","      # TODO(b/127301657): Decay epsilon based on global step, cf. cl/188907839\n","      tf_agent = dqn_agent.DqnAgent(\n","          tf_env.time_step_spec(),\n","          tf_env.action_spec(),\n","          q_network=q_net,\n","          epsilon_greedy=epsilon_greedy,\n","          n_step_update=n_step_update,\n","          target_update_tau=target_update_tau,\n","          target_update_period=target_update_period,\n","          optimizer=optimizer,\n","          td_errors_loss_fn=td_errors_loss_fn,\n","          gamma=gamma,\n","          reward_scale_factor=reward_scale_factor,\n","          gradient_clipping=gradient_clipping,\n","          debug_summaries=debug_summaries,\n","          summarize_grads_and_vars=summarize_grads_and_vars,\n","          check_numerics=check_numerics,\n","          train_step_counter=global_step)\n","    elif agent == 'PPO':\n","      # TODO: Use ppo_clip_agent which is the proposed above\n","      # tf_agent = ppo_clip_agent.PPOClipAgent(\n","      tf_agent = ppo_agent.PPOAgent(\n","        tf_env.time_step_spec(),\n","        tf_env.action_spec(),\n","        optimizer=optimizer,\n","        actor_net=actor_net,\n","        value_net=value_net,\n","        importance_ratio_clipping=importance_ratio_clipping,\n","        kl_cutoff_factor=kl_cutoff_factor,\n","        kl_cutoff_coef=kl_cutoff_coef,\n","        initial_adaptive_kl_beta=initial_adaptive_kl_beta,\n","        adaptive_kl_target=adaptive_kl_target,\n","        adaptive_kl_tolerance=adaptive_kl_tolerance,\n","        lambda_value=lambda_value,\n","        discount_factor=discount_factor,\n","        entropy_regularization=entropy_regularization,\n","        policy_l2_reg=policy_l2_reg,\n","        value_function_l2_reg=value_function_l2_reg,\n","        # shared_vars_l2_reg=shared_vars_l2_reg,\n","        value_pred_loss_coef=value_pred_loss_coef,\n","        normalize_observations=normalize_observations,\n","        use_gae=use_gae,\n","        use_td_lambda_return=use_td_lambda_return,\n","        normalize_rewards=normalize_rewards,\n","        reward_norm_clipping=reward_norm_clipping,\n","        log_prob_clipping=log_prob_clipping,\n","        gradient_clipping=gradient_clipping,\n","        # value_clipping=value_clipping,\n","        num_epochs=num_epochs,\n","        debug_summaries=debug_summaries,\n","        summarize_grads_and_vars=summarize_grads_and_vars,\n","        check_numerics=check_numerics,\n","        train_step_counter=global_step)\n","    else:\n","      raise NotImplementedError('Other agents than DQN and PPO are not yet implemented')\n","\n","    for steps_per_episode in steps_schedule:\n","\n","      logging.info(\n","        f'Steps per episode equal to {steps_per_episode}'\n","      )\n","\n","      tf_env, eval_tf_env, test_tf_env = generateSplitEnvs(\n","        train,\n","        valid,\n","        test,\n","        window_size,\n","        steps_per_episode,\n","        FEATURE_COLUMNS,\n","        reward_type=reward_type,\n","        max_final_reward=max_final_reward,\n","        max_step_reward=max_step_reward,\n","        num_parallel_environments=num_parallel_environments,\n","        position_as_observation=POSITION_AS_OBSERVATION,\n","        constant_step=False,\n","        is_training=True,\n","        seed=SEED,\n","      )\n","\n","      tf.compat.v2.summary.scalar(\n","        name='step_scheduling', data=steps_per_episode, step=global_step)\n","\n","      eval_metrics_callback.reset()\n","\n","      train_eval(\n","        tf_agent,\n","        num_iterations,\n","        batch_size,\n","        tf_env,\n","        eval_tf_env,\n","        train_metrics,\n","        step_metrics,\n","        eval_metrics,\n","        global_step,\n","        replay_buffer_capacity,\n","        num_parallel_environments,\n","        collect_per_iteration,\n","        train_steps_per_iteration,\n","        train_dir,\n","        saved_model_dir,\n","        eval_summary_writer,\n","        num_eval_episodes,\n","        num_eval_seeds=num_eval_seeds,\n","        eval_metrics_callback=eval_metrics_callback,\n","        train_sequence_length=train_sequence_length,\n","        initial_collect_steps=initial_collect_steps if agent=='DQN' else None,\n","        train_model=TRAIN_MODEL,\n","        use_tf_functions=use_tf_functions,\n","        eval_early_stopping=True,\n","      )\n","\n","      tf.compat.v2.summary.scalar(\n","        name='step_scheduling', data=steps_per_episode, step=global_step)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"output_type":"stream","name":"stderr","text":"INFO:absl: \n\t\t AverageReturn = 0.0\n"}],"source":["# One last evaluation\n","results = evaluate(eval_metrics, eval_tf_env, tf_agent.policy, num_eval_episodes, num_eval_seeds, global_step, eval_summary_writer, summary_prefix='Metrics')"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["env_data = train # valid\n","\n","all_envs = {}\n","\n","full_env = OwnStocksEnv(\n","    df=env_data,\n","    window_size=window_size,\n","    frame_bound=(window_size, len(env_data)),\n","    steps_per_episode=len(env_data) - window_size, # steps_per_episode,\n","    constant_step=True,\n","    is_training=False,\n","    feature_columns=FEATURE_COLUMNS,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n","    reward_type=reward_type,\n","    max_final_reward=max_final_reward,\n","    max_step_reward=max_step_reward,\n",")\n","all_envs['Full eval'] = full_env\n","\n","#TODO: For the is_training=True we have to make that all executions are using same cases\n","step_env = OwnStocksEnv(\n","    df=env_data,\n","    window_size=window_size,\n","    frame_bound=(window_size, len(env_data)),\n","    steps_per_episode=steps_per_episode,\n","    constant_step=True,\n","    is_training=True,\n","    feature_columns=FEATURE_COLUMNS,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n","    reward_type=reward_type,\n","    max_final_reward=max_final_reward,\n","    max_step_reward=max_step_reward,\n",")\n","all_envs[f'Eval step of {steps_per_episode}'] = step_env\n","\n","large_step_env = OwnStocksEnv(\n","    df=env_data,\n","    window_size=window_size,\n","    frame_bound=(window_size, len(env_data)),\n","    steps_per_episode=10 * steps_per_episode,\n","    constant_step=True,\n","    is_training=True,\n","    feature_columns=FEATURE_COLUMNS,\n","    position_as_observation=POSITION_AS_OBSERVATION,\n","    reward_type=reward_type,\n","    max_final_reward=max_final_reward,\n","    max_step_reward=max_step_reward,\n",")\n","all_envs[f'Eval step of {10*steps_per_episode}'] = large_step_env\n","\n","if int(0.1 * steps_per_episode) > 1:\n","    small_step_env = OwnStocksEnv(\n","        df=env_data,\n","        window_size=window_size,\n","        frame_bound=(window_size, len(env_data)),\n","        steps_per_episode=int(0.1 * steps_per_episode),\n","        constant_step=True,\n","        is_training=True,\n","        feature_columns=FEATURE_COLUMNS,\n","        position_as_observation=POSITION_AS_OBSERVATION,\n","        reward_type=reward_type,\n","        max_final_reward=max_final_reward,\n","        max_step_reward=max_step_reward,\n","    )\n","    all_envs[f'Eval step of {int(0.1 * steps_per_episode)}'] = small_step_env"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["from own_stock_env import runAllTestEnv"]},{"cell_type":"code","execution_count":33,"metadata":{"tags":["outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend","outputPrepend"]},"outputs":[{"output_type":"stream","name":"stdout","text":"Testing enviorment Full eval:\nTotal rewards: 7783.67  0.000 (mean  std. dev. of 1 iterations)\nTotal profits: 608.61%  0.000% (mean  std. dev. of 1 iterations)\nTotal revenue ratio: 0.00%  0.000% (mean  std. dev. of 1 iterations)\n--------------------------------------------------\nTesting enviorment Eval step of 5:\nTotal rewards: 2.14  82.127 (mean  std. dev. of 3455 iterations)\nTotal profits: 0.05%  1.189% (mean  std. dev. of 3455 iterations)\nTotal revenue ratio: 20.44%  28.138% (mean  std. dev. of 3455 iterations)\n--------------------------------------------------\nTesting enviorment Eval step of 50:\nTotal rewards: 25.30  261.359 (mean  std. dev. of 345 iterations)\nTotal profits: 0.89%  4.038% (mean  std. dev. of 345 iterations)\nTotal revenue ratio: 12.52%  14.422% (mean  std. dev. of 345 iterations)\n--------------------------------------------------\n"}],"source":["# Apply random policy on env\n","runAllTestEnv(all_envs, select_action_func=full_env.action_space.sample);"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"Testing enviorment Full eval:\nTotal rewards: 13743.53  0.000 (mean  std. dev. of 1 iterations)\nTotal profits: 3169.19%  0.000% (mean  std. dev. of 1 iterations)\nTotal revenue ratio: 0.00%  0.000% (mean  std. dev. of 1 iterations)\n--------------------------------------------------\nTesting enviorment Eval step of 5:\nTotal rewards: 2.15  110.181 (mean  std. dev. of 3455 iterations)\nTotal profits: 0.08%  1.538% (mean  std. dev. of 3455 iterations)\nTotal revenue ratio: 26.94%  32.416% (mean  std. dev. of 3455 iterations)\n--------------------------------------------------\nTesting enviorment Eval step of 50:\nTotal rewards: 53.09  321.614 (mean  std. dev. of 345 iterations)\nTotal profits: 1.25%  4.849% (mean  std. dev. of 345 iterations)\nTotal revenue ratio: 16.59%  19.129% (mean  std. dev. of 345 iterations)\n--------------------------------------------------\n"}],"source":["# Applying long term policy (buy at initial and do not sell) on env\n","from gym_anytrading.envs import Actions \n","\n","def always_buy_func():\n","    return  Actions.Buy.value\n","\n","runAllTestEnv(all_envs, select_action_func=always_buy_func);"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"Testing enviorment Full eval:\nTotal rewards: 1071.96  0.000 (mean  std. dev. of 1 iterations)\nTotal profits: 62.15%  0.000% (mean  std. dev. of 1 iterations)\nTotal revenue ratio: 0.00%  0.000% (mean  std. dev. of 1 iterations)\n--------------------------------------------------\nTesting enviorment Eval step of 5:\nTotal rewards: 0.03  2.367 (mean  std. dev. of 3455 iterations)\nTotal profits: 0.00%  0.145% (mean  std. dev. of 3455 iterations)\nTotal revenue ratio: 1.12%  10.059% (mean  std. dev. of 3455 iterations)\n--------------------------------------------------\nTesting enviorment Eval step of 50:\nTotal rewards: -0.47  11.744 (mean  std. dev. of 345 iterations)\nTotal profits: -0.04%  1.093% (mean  std. dev. of 345 iterations)\nTotal revenue ratio: 0.91%  4.896% (mean  std. dev. of 345 iterations)\n--------------------------------------------------\n"}],"source":["# Applying baseline policy on env\n","# Manual policy used as baseline\n","from gym_anytrading.envs import Positions, Actions\n","\n","rsi_col = 'RSI_14'\n","# rsi_col = 'Close_rsi'\n","rsi_index = full_env.feature_columns.index(rsi_col)\n","\n","# RSI usually is between 0 and 100, here is normalized between -1 and 1\n","# The baseline strategy is buy at 30 and sell at 70 otherwise hold\n","def select_baseline_action(observation, rsi_thresh_buy=-0.6, rsi_thresh_sell=0.4, rsi_index=rsi_index):\n","    # Use only last observation\n","    obs = observation[-1]\n","\n","    position_value = int(obs[-1])\n","    rsi = obs[rsi_index]\n","\n","    if position_value == Positions.Short.value and rsi <= rsi_thresh_buy:\n","        action = Actions.Buy.value\n","    elif position_value == Positions.Long.value and rsi >= rsi_thresh_sell:\n","        action = Actions.Sell.value\n","    else:\n","        # Hold\n","        # if it was in short remain in short because is selling\n","        # if it was in long remain in long because is buying\n","        action = position_value\n","    \n","    return action\n","\n","runAllTestEnv(all_envs, select_action_func=select_baseline_action, use_observation=True, rsi_thresh_buy=0.2, rsi_thresh_sell=0.8);"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["def select_TFEnv_action(TFEnv, policy, done, time_step=None, policy_state=None):\n","    \n","    action_step = policy.action(time_step, policy_state)\n","    # distribution_step = policy._distribution(  # pylint: disable=protected-access\n","    #     time_step, policy_state)\n","    # if distribution_step.action.log_prob(0) > distribution_step.action.log_prob(1):\n","    #     print(distribution_step)\n","    #     print(distribution_step.action.log_prob(0), distribution_step.action.log_prob(1))\n","\n","    # TODO(b/134487572): TF2 while_loop seems to either ignore\n","    # parallel_iterations or doesn't properly propagate control dependencies\n","    # from one step to the next. Without this dep, self.env.step() is called\n","    # in parallel.\n","    with tf.control_dependencies(tf.nest.flatten([time_step])):\n","        next_time_step = TFEnv.step(action_step.action)\n","\n","    policy_state = action_step.state\n","\n","    action = action_step.action.numpy()[0]\n","    # print(action)\n","\n","    done = next_time_step.discount.numpy()[0] == 0\n","    # if done:\n","    #     display(TFEnv.envs[0].max_possible_profit_df.iloc[-1,0])\n","    #     print(TFEnv.envs[0]._total_profit)\n","    #     print(TFEnv.envs[0].calculate_revenue_ratio())\n","\n","    return action, done, next_time_step, policy_state"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["all_tf_envs = {}\n","\n","for key, value in all_envs.items():\n","    all_tf_envs[key] = tf_py_environment.TFPyEnvironment(GymWrapper(value))"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":"Testing enviorment Full eval:\nTotal rewards: 6688.14  0.000 (mean  std. dev. of 1 iterations)\nTotal profits: 118.80%  0.000% (mean  std. dev. of 1 iterations)\nTotal revenue ratio: 0.00%  0.000% (mean  std. dev. of 1 iterations)\n--------------------------------------------------\nTesting enviorment Eval step of 5:\n"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-30-4318bcfe0db0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrunAllTestEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_tf_envs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselect_action_func\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mselect_TFEnv_action\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misTFEnv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf_agent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect_policy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[1;32mc:\\Users\\Lluis\\Desktop\\Machine Learning\\Crypto_trader\\own_stock_env.py\u001b[0m in \u001b[0;36mrunAllTestEnv\u001b[1;34m(all_envs, select_action_func, **kwargs)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0menv_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mall_envs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Testing enviorment {env_name}:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m             \u001b[0mrunTestEnv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mselect_action_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mc:\\Users\\Lluis\\Desktop\\Machine Learning\\Crypto_trader\\own_stock_env.py\u001b[0m in \u001b[0;36mrunTestEnv\u001b[1;34m(env, select_action_func, iterations, use_steps, use_observation, use_model, isTFEnv, policy, seed, **kwargs)\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misTFEnv\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m                 \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_action_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTFEnv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTFEnv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0muse_model\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-28-9d1ce9f92600>\u001b[0m in \u001b[0;36mselect_TFEnv_action\u001b[1;34m(TFEnv, policy, done, time_step, policy_state)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mselect_TFEnv_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTFEnv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0maction_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m# distribution_step = policy._distribution(  # pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#     time_step, policy_state)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py\u001b[0m in \u001b[0;36maction\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_automatic_state_reset\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m       \u001b[0mpolicy_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_reset_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m     \u001b[0mstep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclip_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tf_agents\\utils\\common.py\u001b[0m in \u001b[0;36mwith_check_resource_vars\u001b[1;34m(*fn_args, **fn_kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;31m# We're either in eager mode or in tf.function mode (no in-between); so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;31m# autodep-like behavior is already expected of fn.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mresource_variables_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMISSING_RESOURCE_VARIABLES_ERROR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tf_agents\\policies\\tf_policy.py\u001b[0m in \u001b[0;36m_action\u001b[1;34m(self, time_step, policy_state, seed)\u001b[0m\n\u001b[0;32m    467\u001b[0m     \"\"\"\n\u001b[0;32m    468\u001b[0m     \u001b[0mseed_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSeedStream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msalt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ppo_policy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 469\u001b[1;33m     \u001b[0mdistribution_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpolicy_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    470\u001b[0m     actions = tf.nest.map_structure(\n\u001b[0;32m    471\u001b[0m         \u001b[1;32mlambda\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mreparameterized_sampling\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed_stream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tf_agents\\agents\\ppo\\ppo_policy.py\u001b[0m in \u001b[0;36m_distribution\u001b[1;34m(self, time_step, policy_state, training)\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;31m# Actor network outputs nested structure of distributions or actions.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     actions_or_distributions, policy_state = self._apply_actor_network(\n\u001b[1;32m--> 165\u001b[1;33m         time_step, policy_state, training=training)\n\u001b[0m\u001b[0;32m    166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_to_distribution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_or_distribution\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tf_agents\\agents\\ppo\\ppo_policy.py\u001b[0m in \u001b[0;36m_apply_actor_network\u001b[1;34m(self, time_step, policy_state, training)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_observation_normalizer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m       observation = self._observation_normalizer.normalize(\n\u001b[1;32m--> 148\u001b[1;33m           time_step.observation)\n\u001b[0m\u001b[0;32m    149\u001b[0m       time_step = ts.TimeStep(time_step.step_type, time_step.reward,\n\u001b[0;32m    150\u001b[0m                               time_step.discount, observation)\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tf_agents\\utils\\tensor_normalizer.py\u001b[0m in \u001b[0;36mnormalize\u001b[1;34m(self, tensor, clip_value, center_mean, variance_epsilon)\u001b[0m\n\u001b[0;32m    151\u001b[0m               t, -clip_value, clip_value, name='clipped_normalized_tensor')\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mnormalized_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_clip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalized_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m     normalized_tensor = tf.nest.pack_sequence_as(self._tensor_spec,\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    570\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tf_agents\\utils\\tensor_normalizer.py\u001b[0m in \u001b[0;36m_clip\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    149\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_clip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    150\u001b[0m           return tf.clip_by_value(\n\u001b[1;32m--> 151\u001b[1;33m               t, -clip_value, clip_value, name='clipped_normalized_tensor')\n\u001b[0m\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[0mnormalized_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_clip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalized_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\ops\\clip_ops.py\u001b[0m in \u001b[0;36mclip_by_value\u001b[1;34m(t, clip_value_min, clip_value_max, name)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# Go through list of tensors, for each value in each tensor clip\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[0mt_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclip_value_max\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;31m# Assert that the shape is compatible with the initial shape,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# to prevent unintentional broadcasting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m~\\Anaconda3\\envs\\crypto_trader\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_math_ops.py\u001b[0m in \u001b[0;36mminimum\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   5977\u001b[0m       _result = _pywrap_tensorflow.TFE_Py_FastPathExecute(\n\u001b[0;32m   5978\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Minimum\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5979\u001b[1;33m         tld.op_callbacks, x, y)\n\u001b[0m\u001b[0;32m   5980\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5981\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["runAllTestEnv(all_tf_envs, select_action_func=select_TFEnv_action, use_model=True, isTFEnv=True, policy=tf_agent.collect_policy)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"language_info":{"name":"python","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.6-final"},"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"kernelspec":{"name":"python37664bitcryptotraderconda69cc994ed1944dadbb053620b665a6b3","display_name":"Python 3.7.6 64-bit ('crypto_trader': conda)"}},"nbformat":4,"nbformat_minor":2}